{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2 - Spark RDD Lab\n",
    "\n",
    "- Họ và tên: Nguyễn Minh Quang\n",
    "\n",
    "- MSSV: 22133045"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Khởi tạo SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: findspark in /opt/conda/lib/python3.11/site-packages (2.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "findspark.find()\n",
    "\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tạo một RDD từ một collection\n",
    "### 1.1 Tạo một Python list gồm các số từ 1 đến 100 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100]\n"
     ]
    }
   ],
   "source": [
    "numb = list(range(1, 101))\n",
    "print(numb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Tạo một RDD từ Python list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbRDD = sc.parallelize(numb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. In ra kiểu của `numbRDD`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The type of RDD is <class 'pyspark.rdd.RDD'>\n"
     ]
    }
   ],
   "source": [
    "print(\"The type of RDD is\", type(numbRDD))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tạo một RDD từ một external dataset\n",
    "### 2.1. Tạo một RDD từ một file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = './README.md' # File readme của thư mục cài đặt Spark\n",
    "fileRDD = sc.textFile(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Kiểm tra kiểu của `fileRDD`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file type of fileRDD is <class 'pyspark.rdd.RDD'>\n",
      "Partition 0: ['# Apache Spark', '', 'Spark is a unified analytics engine for large-scale data processing. It provides', 'high-level APIs in Scala, Java, Python, and R, and an optimized engine that', 'supports general computation graphs for data analysis. It also supports a', 'rich set of higher-level tools including Spark SQL for SQL and DataFrames,', 'MLlib for machine learning, GraphX for graph processing,', 'and Structured Streaming for stream processing.', '', '<https://spark.apache.org/>', '', '[![Jenkins Build](https://amplab.cs.berkeley.edu/jenkins/job/spark-master-test-sbt-hadoop-2.7-hive-2.3/badge/icon)](https://amplab.cs.berkeley.edu/jenkins/job/spark-master-test-sbt-hadoop-2.7-hive-2.3)', '[![AppVeyor Build](https://img.shields.io/appveyor/ci/ApacheSoftwareFoundation/spark/master.svg?style=plastic&logo=appveyor)](https://ci.appveyor.com/project/ApacheSoftwareFoundation/spark)', '[![PySpark Coverage](https://img.shields.io/badge/dynamic/xml.svg?label=pyspark%20coverage&url=https%3A%2F%2Fspark-test.github.io%2Fpyspark-coverage-site&query=%2Fhtml%2Fbody%2Fdiv%5B1%5D%2Fdiv%2Fh1%2Fspan&colorB=brightgreen&style=plastic)](https://spark-test.github.io/pyspark-coverage-site)', '', '', '## Online Documentation', '', 'You can find the latest Spark documentation, including a programming', 'guide, on the [project web page](https://spark.apache.org/documentation.html).', 'This README file only contains basic setup instructions.', '', '## Building Spark', '', 'Spark is built using [Apache Maven](https://maven.apache.org/).', 'To build Spark and its example programs, run:', '', '    ./build/mvn -DskipTests clean package', '', '(You do not need to do this if you downloaded a pre-built package.)', '', 'More detailed documentation is available from the project site, at', '[\"Building Spark\"](https://spark.apache.org/docs/latest/building-spark.html).', '', 'For general development tips, including info on developing Spark using an IDE, see [\"Useful Developer Tools\"](https://spark.apache.org/developer-tools.html).', '', '## Interactive Scala Shell', '', 'The easiest way to start using Spark is through the Scala shell:', '', '    ./bin/spark-shell', '', 'Try the following command, which should return 1,000,000,000:', '', '    scala> spark.range(1000 * 1000 * 1000).count()', '', '## Interactive Python Shell', '', 'Alternatively, if you prefer Python, you can use the Python shell:']\n",
      "Partition 1: ['', '    ./bin/pyspark', '', 'And run the following command, which should also return 1,000,000,000:', '', '    >>> spark.range(1000 * 1000 * 1000).count()', '', '## Example Programs', '', 'Spark also comes with several sample programs in the `examples` directory.', 'To run one of them, use `./bin/run-example <class> [params]`. For example:', '', '    ./bin/run-example SparkPi', '', 'will run the Pi example locally.', '', 'You can set the MASTER environment variable when running examples to submit', 'examples to a cluster. This can be a mesos:// or spark:// URL,', '\"yarn\" to run on YARN, and \"local\" to run', 'locally with one thread, or \"local[N]\" to run locally with N threads. You', 'can also use an abbreviated class name if the class is in the `examples`', 'package. For instance:', '', '    MASTER=spark://host:7077 ./bin/run-example SparkPi', '', 'Many of the example programs print usage help if no params are given.', '', '## Running Tests', '', 'Testing first requires [building Spark](#building-spark). Once Spark is built, tests', 'can be run using:', '', '    ./dev/run-tests', '', 'Please see the guidance on how to', '[run tests for a module, or individual tests](https://spark.apache.org/developer-tools.html#individual-tests).', '', 'There is also a Kubernetes integration test, see resource-managers/kubernetes/integration-tests/README.md', '', '## A Note About Hadoop Versions', '', 'Spark uses the Hadoop core library to talk to HDFS and other Hadoop-supported', 'storage systems. Because the protocols have changed in different versions of', 'Hadoop, you must build Spark against the same version that your cluster runs.', '', 'Please refer to the build documentation at', '[\"Specifying the Hadoop Version and Enabling YARN\"](https://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version-and-enabling-yarn)', 'for detailed guidance on building for a particular distribution of Hadoop, including', 'building for particular Hive and Hive Thriftserver distributions.', '', '## Configuration', '', 'Please refer to the [Configuration Guide](https://spark.apache.org/docs/latest/configuration.html)', 'in the online documentation for an overview on how to configure Spark.', '', '## Contributing', '', 'Please review the [Contribution to Spark guide](https://spark.apache.org/contributing.html)', 'for information on how to get started contributing to the project.']\n"
     ]
    }
   ],
   "source": [
    "print(\"The file type of fileRDD is\", type(fileRDD))\n",
    "\n",
    "partitions = fileRDD.glom().collect()\n",
    "for i, part in enumerate(partitions):\n",
    "    print(f\"Partition {i}: {part}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Phân hoạch dữ liệu\n",
    "### 3.1. Kiểm tra số partition của `fileRDD`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions in fileRDD is 2\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of partitions in fileRDD is\", fileRDD.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Tạo `fileRDD_part` từ `file_path` với 5 partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileRDD_part = sc.textFile(file_path, minPartitions = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Kiểm tra số partition của `fileRDD_part`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions in fileRDD_part is 5\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of partitions in fileRDD_part is\", fileRDD_part.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Map and Collect\n",
    "### 4.1. Tạo một map() transformation để tính lũy thừa bậc 3 của các phần tử trong `numbRDD`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cubedRDD = numbRDD.map(lambda x: x**3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Collect kết quả"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers_all = cubedRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. In ra các phần tử từ `numbers_all`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "8\n",
      "27\n",
      "64\n",
      "125\n",
      "216\n",
      "343\n",
      "512\n",
      "729\n",
      "1000\n",
      "1331\n",
      "1728\n",
      "2197\n",
      "2744\n",
      "3375\n",
      "4096\n",
      "4913\n",
      "5832\n",
      "6859\n",
      "8000\n",
      "9261\n",
      "10648\n",
      "12167\n",
      "13824\n",
      "15625\n",
      "17576\n",
      "19683\n",
      "21952\n",
      "24389\n",
      "27000\n",
      "29791\n",
      "32768\n",
      "35937\n",
      "39304\n",
      "42875\n",
      "46656\n",
      "50653\n",
      "54872\n",
      "59319\n",
      "64000\n",
      "68921\n",
      "74088\n",
      "79507\n",
      "85184\n",
      "91125\n",
      "97336\n",
      "103823\n",
      "110592\n",
      "117649\n",
      "125000\n",
      "132651\n",
      "140608\n",
      "148877\n",
      "157464\n",
      "166375\n",
      "175616\n",
      "185193\n",
      "195112\n",
      "205379\n",
      "216000\n",
      "226981\n",
      "238328\n",
      "250047\n",
      "262144\n",
      "274625\n",
      "287496\n",
      "300763\n",
      "314432\n",
      "328509\n",
      "343000\n",
      "357911\n",
      "373248\n",
      "389017\n",
      "405224\n",
      "421875\n",
      "438976\n",
      "456533\n",
      "474552\n",
      "493039\n",
      "512000\n",
      "531441\n",
      "551368\n",
      "571787\n",
      "592704\n",
      "614125\n",
      "636056\n",
      "658503\n",
      "681472\n",
      "704969\n",
      "729000\n",
      "753571\n",
      "778688\n",
      "804357\n",
      "830584\n",
      "857375\n",
      "884736\n",
      "912673\n",
      "941192\n",
      "970299\n",
      "1000000\n"
     ]
    }
   ],
   "source": [
    "for numb in numbers_all:\n",
    "\tprint(numb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Filter and Count\n",
    "### 5.1 Áp dụng filter trên `fileRDD` để chọn ra những dòng có từ `Spark`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileRDD_filter = fileRDD.filter(lambda line: 'Spark' in line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Cho biết có bao nhiêu dòng từ `fileRDD`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of lines with the keyword Spark is 19\n"
     ]
    }
   ],
   "source": [
    "print(\"The total number of lines with the keyword Spark is\", fileRDD_filter.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. In 5 dòng đầu của `fileRDD`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Apache Spark\n",
      "Spark is a unified analytics engine for large-scale data processing. It provides\n",
      "rich set of higher-level tools including Spark SQL for SQL and DataFrames,\n",
      "[![PySpark Coverage](https://img.shields.io/badge/dynamic/xml.svg?label=pyspark%20coverage&url=https%3A%2F%2Fspark-test.github.io%2Fpyspark-coverage-site&query=%2Fhtml%2Fbody%2Fdiv%5B1%5D%2Fdiv%2Fh1%2Fspan&colorB=brightgreen&style=plastic)](https://spark-test.github.io/pyspark-coverage-site)\n",
      "You can find the latest Spark documentation, including a programming\n"
     ]
    }
   ],
   "source": [
    "for line in fileRDD_filter.take(5): \n",
    "  print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ReduceBykey and Collect  \n",
    "### 6.1. Tạo một `PairRDD` tên `Rdd` như sau:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rdd = sc.parallelize([(1,2),(3,4),(3,6),(4,5)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2. Áp dụng `reduceByKey()` operation trên Rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rdd_Reduced = Rdd.reduceByKey(lambda x, y: x + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3. Duyệt và in ra kết quả"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key 1 has 2 Counts\n",
      "Key 3 has 10 Counts\n",
      "Key 4 has 5 Counts\n"
     ]
    }
   ],
   "source": [
    "for num in Rdd_Reduced.collect(): \n",
    "    print(\"Key {} has {} Counts\".format(num[0], num[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. SortByKey and Collect\n",
    "### 7.1. Sắp xếp `Rdd_Reduced` theo khóa giảm dần"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rdd_Reduced_Sort = Rdd_Reduced.sortByKey(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2. Duyệt và in ra kết quả"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key 4 has 5 Counts\n",
      "Key 3 has 10 Counts\n",
      "Key 1 has 2 Counts\n"
     ]
    }
   ],
   "source": [
    "for num in Rdd_Reduced_Sort.collect():\n",
    "    print(\"Key {} has {} Counts\".format(num[0], num[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. CountingBykeys\n",
    "### 8.1. Đếm số key khác nhau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = Rdd.countByKey()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Kiểm tra kiểu của biến `total`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The type of total is <class 'collections.defaultdict'>\n"
     ]
    }
   ],
   "source": [
    "print(\"The type of total is\", type(total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2. Duyệt và in ra kết quả"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key 1 has 1 counts\n",
      "key 3 has 2 counts\n",
      "key 4 has 1 counts\n"
     ]
    }
   ],
   "source": [
    "for k, v in total.items(): \n",
    "  print(\"key\", k, \"has\", v, \"counts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Tạo biến `baseRDD` từ file và thực hiện các transform\n",
    "### 9.1 Tạo biến `baseRDD` từ `file_path`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = './README.md'\n",
    "baseRDD = sc.textFile(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2. Biến đổi các dòng trong `baseRDD` thành các từ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitRDD = baseRDD.flatMap(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3. Đếm tổng số từ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words in splitRDD: 495\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of words in splitRDD:\", splitRDD.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Loại bỏ stop word và áp dụng reduce opperator trên tập dữ liệu\n",
    "### 10.1. Chuyển các từ sang dạng chữ thường (lower case) và loại bỏ các stop words từ biến `stop_words`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = ['a', 'an', 'the', 'is', 'be', 'was', 'were', 'it', 'this', 'that', 'but', 'if', 'or', 'not']\n",
    "splitRDD_no_stop = splitRDD.filter(lambda x: x.lower() not in stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2. Tạo các tuple có dạng (word, 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitRDD_no_stop_words = splitRDD_no_stop.map(lambda w: (w, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.3. Đếm tần số của mỗi từ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultRDD = splitRDD_no_stop_words.reduceByKey(lambda x, y: x + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. In tần số của mỗi từ\n",
    "### 11.1. Hiển thị 10 từ đầu và tần số của nó trong `resultRDD`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('#', 1)\n",
      "('analytics', 1)\n",
      "('for', 12)\n",
      "('Scala,', 1)\n",
      "('Python,', 2)\n",
      "('and', 9)\n",
      "('optimized', 1)\n",
      "('supports', 2)\n",
      "('general', 2)\n",
      "('analysis.', 1)\n"
     ]
    }
   ],
   "source": [
    "for word in resultRDD.take(10):\n",
    "\tprint(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.2 Hoán đổi giữa key và value trong `resultRDD`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultRDD_swap = resultRDD.map(lambda x: (x[1], x[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.3. Sắp xếp các key theo thứ tự giảm dần"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultRDD_swap_sort = resultRDD_swap.sortByKey(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.4. Hiển thị 10 từ xuất hiện nhiều nhất trong `resultRDD_swap_sort`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to,16\n",
      "Spark,14\n",
      "for,12\n",
      "and,9\n",
      "##,9\n",
      "on,7\n",
      "run,7\n",
      "can,6\n",
      "of,5\n",
      "in,5\n"
     ]
    }
   ],
   "source": [
    "for word in resultRDD_swap_sort.take(10):\n",
    "\tprint(\"{},{}\". format(word[1], word[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
